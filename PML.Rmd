---
title: "Practical ML Project"
author: "Adam Yao Guang Chen"
date: "28 January 2017"
output:
  html_document:
    toc: yes
  pdf_document:
    toc: yes
---

## Introduction
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

## Initialization
```{r, cache = T}
library(RCurl)
library(caret)
library(randomForest)
library(ggplot2)
library(rpart)
library(rpart.plot)
```

## Data Processing 
```{r, cache = T}
# Download data
URL_Train <-"http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
URL_Test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

File_Train <- "./data/pml-training.csv"
File_Test  <- "./data/pml-testing.csv"

if (!file.exists("./data")) {
  dir.create("./data")
}

download.file(URL_Train, File_Train)
download.file(URL_Test, File_Test)

# Read Data
TrainData = read.csv(File_Train)
TestData = read.csv(File_Test)

dim(TrainData)
dim(TestData)
#names(TrainData)
```  
There are 19622 & 20 observations in training & testing data respectively. The number of variables is 160. The "classe" variable in the training set is the outcome to predict.


## Data cleansing
1) Remove all columns with NA values in test dataset
2) Keep numeric columns & Keep Classes for TrainData
2) Remove columns not much ralevant to the final preditions
```{r}
# remove columns with NA in TestData
SelectedColumns = colSums(is.na(TestData)) == 0
TrainData = TrainData[, SelectedColumns]
TestData = TestData[, SelectedColumns]

# Keep numeric columns & Keep Classes for TrainData
classe <- TrainData$classe
TrainData <- TrainData[, sapply(TrainData, is.numeric)]
TestData <- TestData[, sapply(TestData, is.numeric)]
TrainData$classe <- classe

# Remove columns not much ralevant to the final preditions
TrainData <- TrainData[, !grepl("^X|timestamp|window", names(TrainData))]
TestData <- TestData[, !grepl("^X|timestamp|window", names(TestData))]

dim(TrainData)
dim(TestData)
```
There were 53 variables remained.


##  Boosting Model
Fit model with boosting algorithm and 10-fold cross validation to predict "classe" by other variables.    
```{r boost, eval = T}
set.seed(1104)
Boosting <- train(classe ~ ., method = "gbm", data = TrainData, verbose = F, trControl = trainControl(method = "cv", number = 10))

Boosting
plot(Boosting, ylim = c(0.7, 1))
```

The boosting algorithm generated a very good model with accuracy of 96.39% and the estimated out-of-sample error is 4.61%. 


## Random Forests model   
Fit model with random forests algorithm and 10-fold cross validation to predict "classe" by other variables.  
```{r rf, eval = T}
set.seed(1104)
RandomForests <- train(classe ~ ., method = "rf", data = TrainData, importance = T, trControl = trainControl(method = "cv", number = 10))

#Plot accuracy of this model on the scale of [0.95, 1].  
RandomForests
plot(RandomForests, ylim = c(0.95, 1))
#The Random Forests algorithm generated an excellent model with accuracy of 99.567% and the estimated out-of-sample error is 0.433%.   

RandomForests$finalModel
# Random Foresters predictions
(prediction <- as.character(predict(RandomForests, TestData)))
```

## Predictions with Random Forests
The random forests algorithm generated a more accurate model with accuracy close to 1. So we'll choose Random Forests model for prediction.

The final line is predit results of test data.






